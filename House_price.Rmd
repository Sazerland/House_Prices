---
title: "N&N_ML_Lab1"
author: "Natasha Savic, Nika Tamaio Flores"
date: "07/02/2018"
output: pdf_document
editor_options: 
  chunk_output_type: console
---


#Housing Price Prediction - Ames, Iowa, United States

#Introduction
If we were to ask a home buyer to explain their dream house, they would probably not begin with the height of the garage ceiling or the proximity to a highway. The questions that really concern home buys rather go into the direction of which the determinants of housing prices are. Would it be possible to explain the house of a price by considering only some particular features of a house like e.g. the neighbourhood?

Good, there are aspiring data scientists like us to solve such questions and, even better, provide a model that will be able to predict housing prices!
So brace yourself, pour yourself a big glass of beer and enjoy our predictive analysis of housing prices in Ames!

##Libraries
```{r libs}
library(ggplot2)
library(plyr)
library(dplyr)
library(moments)
library(glmnet)
library(caret)
library(e1071)
library(tidyr)
library(mice)
library(MASS)
```


##Data Reading
In this part, we will clean and read the data. In order to facilitate the selection process, we will convert all values into numeric.

```{r train data}
setwd('/Users/natasha/Desktop/ML II')
df <- read.csv('train.csv', sep=',', header=T)
```

##Check NA's

```{r check na}
na.cols <- which(colSums(is.na(df)) > 0)
paste('There are', length(na.cols), 'columns with missing values')
sort(colSums(sapply(df[na.cols], is.na)), decreasing = TRUE)
```

#Data Preparation
##Data Cleaning - Train Dataset

The visualisation part of this section has been adopted by [missingDataLab](http://web.maths.unsw.edu.au/~dwarton/missingDataLab.html).

```{r na plot}
#Check Missing Values
library(VIM)
#Now we will check the proportion of missing values plotting them 
df_aggr = aggr(df, col=mdc(1:2), numbers=TRUE, sortVars=TRUE, labels=names(df), cex.axis=.7, gap=3, ylab=c("Proportion of missingness","Missingness Pattern"))

```

Cool! Now we see that there are particular features we need to focus on when cleaning the data. As the plot illustrates, critical features include PoolQC, MiscFeature, Alley, Fence and FireplaceQu and convert features to the right data type.

```{r filling na}
#take care of critical Columns manually, rest using mice
df$Alley = factor(df$Alley, levels=c(levels(df$Alley), "None")) #appending another level "None" to exisiting levels
df$Alley[is.na(df$Alley)] = "None" #replace NA's by "None"

# Fence : NA means "no fence"
df$Fence = factor(df$Fence, levels=c(levels(df$Fence), "No"))
df$Fence[is.na(df$Fence)] = "No"

# FireplaceQu : NA means "no fireplace"
df$FireplaceQu = factor(df$FireplaceQu, levels=c(levels(df$FireplaceQu), "No")) #factor the data
df$FireplaceQu[is.na(df$FireplaceQu)] = "No"

# MiscFeature : NA = "no misc feature"
df$MiscFeature = factor(df$MiscFeature, levels=c(levels(df$MiscFeature), "No"))
df$MiscFeature[is.na(df$MiscFeature)] = "No"

# PoolQC : data description says NA means "no pool"
df$PoolQC = factor(df$PoolQC, levels=c(levels(df$PoolQC), "No"))
df$PoolQC[is.na(df$PoolQC)] = "No"


#Factorize features. Some numerical features are actually really categories
df$MSSubClass <- as.factor(df$MSSubClass)
df$MoSold <- as.factor(df$MoSold)
```

##Data Imputation

Cool, that was easy! We will now proceed with removing those missing values and make another plot to cross-validate. For this we will use the package mice. The Function mice() is a Markov Chain Monte Carlo (MCMC) method that uses the correlation structure of the data and imputes missing values for each incomplete variable 'm' times by regression of incomplete variables on the other variables iteratively. However, we will not apply 'mice' on every feature but rather "clean" critical features manually and perform imputations for the remaining features using the quickpred() function to leverage computational time with accuracy. 

```{r mice, results="hide"}
library(mice)
imp = mice(df, pred=quickpred(df, minpuc = 0.25, exclude = c("PoolQC", "MiscFeature", "Alley", "Fence", "FireplaceQu", "LotFrontage", "SalePrice")), maxit = 1, method = 'cart')
summary(imp)
completedData <- complete(imp,1)
```

To doublecheck the "cleaniliness" of our data we will plot it once more. 

```{r na plot check}
df_aggr = aggr(completedData, col=mdc(1:2), numbers=TRUE, sortVars=TRUE, labels=names(completedData), cex.axis=.7, gap=3, ylab=c("Proportion of missingness","Missingness Pattern"))
```

The output shows that there are no missing values for our data anymore hence, we can continue with our analysis and move on to enrichting our Dataset with further predictors.

##Data Cleaning Test Dataset

We will do now the same for the test dataset to ensure consistency. We will name the test dataset as *dft* whereas the training dataset represents *df*. 
```{r test data}
dft <- read.csv('test.csv', sep=',', header=T)
```

###Check NA's

```{r test check na}
na.cols <- which(colSums(is.na(dft)) > 0)
paste('There are', length(na.cols), 'columns with missing values')
sort(colSums(sapply(dft[na.cols], is.na)), decreasing = TRUE)
```


```{r na plot}
#Check Missing Values
dft_aggr = aggr(dft, col=mdc(1:2), numbers=TRUE, sortVars=TRUE, labels=names(dft), cex.axis=.7, gap=3, ylab=c("Proportion of missingness","Missingness Pattern"))

```

Similar picture as for our training dataset however, we see small differences here. For example, the feature "LotFrontage" has a significant number of NA's in the test dataset hence, we chose to clean this variable manually too. 

```{r test fill na}
#take care of critical columns manually, rest using mice

dft$Alley = factor(dft$Alley, levels=c(levels(dft$Alley), "None")) #appending another level "None" to exisiting levels
dft$Alley[is.na(dft$Alley)] = "None" #replace NA's by "None"

# Fence : NA means "no fence"
dft$Fence = factor(dft$Fence, levels=c(levels(dft$Fence), "No"))
dft$Fence[is.na(dft$Fence)] = "No"

# FireplaceQu : NA means "no fireplace"
dft$FireplaceQu = factor(dft$FireplaceQu, levels=c(levels(dft$FireplaceQu), "No")) 
dft$FireplaceQu[is.na(dft$FireplaceQu)] = "No"

# MiscFeature : NA = "no misc feature"
dft$MiscFeature = factor(dft$MiscFeature, levels=c(levels(dft$MiscFeature), "No"))
dft$MiscFeature[is.na(dft$MiscFeature)] = "No"

# PoolQC : data description says NA means "no pool"
dft$PoolQC = factor(dft$PoolQC, levels=c(levels(dft$PoolQC), "No"))
dft$PoolQC[is.na(dft$PoolQC)] = "No"

# LotFrontage : NA most likely means no lot frontage
dft$LotFrontage[is.na(dft$LotFrontage)] <- 0

#Factorize features. Some numerical features are actually categories
dft$MSSubClass <- as.factor(dft$MSSubClass)
dft$MoSold <- as.factor(dft$MoSold)
```

##Data Imputation Test Dataset

```{r test mice, results="hide"}
library(mice)
impt = mice(dft, pred=quickpred(dft, minpuc = 0.25, exclude = c("PoolQC", "MiscFeature", "Alley", "Fence", "FireplaceQu", "LotFrontage", "SalePrice")), maxit = 1, method = 'cart')
summary(impt)
completedDataTest <- complete(impt,1)
```

Doublechecking the "cleaniliness" of our data.

```{r test check na plot}
#An intermediate plot will show that utilities still contains NA's hence, we will remove them manually

df_aggr = aggr(completedDataTest, col=mdc(1:2), numbers=TRUE, sortVars=TRUE, labels=names(completedDataTest), cex.axis=.7, gap=3, ylab=c("Proportion of missingness","Missingness Pattern"))
df_aggr
```

This diagnostic plot illustrates that there is still one variable that still causes NA's. We will check it manually and find out that it only has one value. As this feature contains almost no information, we will remove it. Other than that, our datasets seem pretty clean and we're good to jump onto the more interesting parts like feature engineering and modeling!

```{r test Utilities}
# Remove the Utilities feature from the dataset (It only has one value)
completedData <- completedData[,-which(names(completedData) == "Utilities")]
completedDataTest <- completedDataTest[,-which(names(completedDataTest) == "Utilities")]

```


##Skewness
Before running a linear model we need to observe how our target variable behaves. Hence we will plot the data points of *SalePrice* in order to check skewness.
```{r check skew}
#Get data frame of SalePrice and log(SalePrice + 1) for plotting
dfl <- rbind(data.frame(version="log(price+1)",x=log(completedData$SalePrice + 1)), data.frame(version="price",x=completedData$SalePrice))

ggplot(data=dfl) +
  facet_wrap(~version,ncol=2,scales="free_x") +
  geom_histogram(aes(x=x), bins = 50)
```

Our intuition proved right and indeed we could observe skewness in the target variable. In order to counteract this and obtain a behavior that follows more a normal distribution we will transform the target value by applying the natural log to our response variable *SalePrice*.

```{r Log transform the target for official scoring}
# Log transform the target for official scoring
completedData$SalePrice <- log1p(completedData$SalePrice)
```

The same "skewness" observed in the target variable also affects other variables. To facilitate the application of the regression model we are going to also eliminate this skewness. For numeric feature with excessive skewness, perform log transformation.

```{r remove skew}

column_types <- sapply(names(completedData),function(x){class(completedData[[x]])})
numeric_columns <-names(column_types[column_types != "factor"])

# skew of each variable
skew <- sapply(numeric_columns,function(x){skewness(completedData[[x]],na.rm = T)})
# transform all variables above a threshold skewness.

#Testing different thresholds might lead to improved results 
skew <- skew[skew > 0.75]
for(x in names(skew)) {
  completedData[[x]] <- log(completedData[[x]] + 1)
}

```

Performing the same for the test data.

```{r remove skew test}
column_types <- sapply(names(completedDataTest),function(x){class(completedDataTest[[x]])})
numeric_columns <-names(column_types[column_types != "factor"])

skew <- sapply(numeric_columns,function(x){skewness(completedDataTest[[x]],na.rm = T)})
skew <- skew[skew > 0.75]
for(x in names(skew)) {
  completedDataTest[[x]] <- log(completedDataTest[[x]] + 1)
}
```

##Removing Outliers

```{r remove outliers}
remove_outliers <- function(x, na.rm = TRUE, ...) {
  qnt <- quantile(x, probs=c(.25, .75), na.rm = na.rm, ...)
  H <- 1.5 * IQR(x, na.rm = na.rm)
  y <- x
  y
}

feature_classes <- sapply(names(completedData),function(x){class(completedData[[x]])})

num_feats <-names(feature_classes[feature_classes != "factor"])
categorical <- names(feature_classes[feature_classes == "factor"])

num_df <- completedData[num_feats]
categorical_df <- completedData[categorical]

completedData1 <- sapply(names(num_df),function(x){remove_outliers(num_df[[x]])}) 

completedData <- cbind(completedData1, categorical_df)
```

#Feature Engineering
Oftentimes information is hidden within the data such as some particulary valueable insight is hidden between the lines of a good book. To extract such hidden information we will perform feature engineering and create new possible predictors which could help us to build a better model. For some particular new features we chose to add descriptions to illustrate the logic behind combining and extracting information. 

**Refurb**
*If Year Built is different from Year Remodeled then we can assume there was some refurbishment, the refurbishment will likely increase the property value*

```{r refurb}
#for train set
completedData = completedData %>%
  mutate(Refurb = ifelse(YearBuilt != YearRemodAdd, 1, 0))

#for test set
completedDataTest = completedDataTest %>%
  mutate(Refurb = ifelse(YearBuilt != YearRemodAdd, 1, 0))

```

**Seasonality**
*Seasonality (binary): We can assume cyclical events where the month and year sold had an effect on the house price. For instance, in low interest rate cycles (e.g 2015-2017) the prices of houses are higher than in low interest periods. Since this particular dataset focusses on the years 2006-2010 we need to do some research and assign categories according to the pricing development during tis time. Such events are also colled unsystmatic since they are influenced by external market forces and push prices out of their market equilibrium.*

Additional Context (Wikipedia)[https://en.wikipedia.org/wiki/United_States_housing_bubble]: The United States housing bubble was a real estate bubble affecting over half of the U.S. states. Housing prices peaked in early 2006, started to decline in 2006 and 2007, and reached new lows in 2012. On December 30, 2008, the Case???Shiller home price index reported its largest price drop in its history. The credit crisis resulting from the bursting of the housing bubble is???according to general consensus???an important cause of the 2007???2009 recession in the United States.

```{r season, message=FALSE}
#check levels of month & year sold
levels(df$MoSold) # 1 - 12 indicating Jan - Dec
levels(df$YrSold) #2006 - 2010

#for train set
completedData = completedData %>%
  mutate(Season = ifelse(YrSold  == 2006 & MoSold != c(1, 2, 11, 12), "A", 
                         ifelse(YrSold  == 2006 & MoSold == c(1, 2, 11, 12),"B",
                         ifelse(YrSold  != c(2008, 2009) & MoSold != c(1, 2, 11, 12),"C",
                         ifelse(YrSold  == c(2008, 2009),"F", "D")))))

completedData$Season = as.factor(completedData$Season)


#for test set
completedDataTest = completedDataTest %>%
  mutate(Season = ifelse(YrSold  == 2006 & MoSold != c(1, 2, 11, 12), "A", 
                         ifelse(YrSold  == 2006 & MoSold == c(1, 2, 11, 12),"B",
                         ifelse(YrSold  != c(2008, 2009) & MoSold != c(1, 2, 11, 12),"C",
                         ifelse(YrSold  == c(2008, 2009),"F", "D")))))

completedDataTest$Season = as.factor(completedDataTest$Season)

```

*Additional Features*
```{r Train add feat}
#total floor area
completedData$FlrSF <- sum(completedData$X1stFlrSF, completedData$X2ndFlrSF)

#total number of full bathrooms
completedData$TotFullBath <- sum(completedData$BsmtFullBath, completedData$FullBath) 

#total number of half bathrooms
completedData$TotSmallBath <- sum(completedData$BsmtHalfBath, completedData$HalfBath)

#recoding values
levels(completedData$CentralAir) <- c(0,1)
completedData$CentralAir <- as.numeric(as.character(completedData$CentralAir))
#converting to numeric
completedData$BsmntUnf <- completedData$BsmtUnfSF/completedData$TotalBsmtSF 

#Unfinished basement area
completedData$BsmntUnf[is.na(completedData$BsmntUnf)] <- 0

#Unfinished house area
completedData$AreaUnf <- completedData$LowQualFinSF/completedData$X1stFlrSF+completedData$X2ndFlrSF 

#Garage and house built at the same time
completedData$TotBuild <- completedData$GarageYrBlt-completedData$YearBuilt

#Total Completed House Area
completedData$TotArea <- sum(completedData$BsmtFinSF1, completedData$BsmtFinSF2, completedData$LotArea, completedData$TotalBsmtSF, completedData$GrLivArea)

#Age of the house when sold
completedData$Age <- completedData$YrSold-completedData$YearBuilt

#Years since remodeling until the house was sold
completedData$RemodSold <- completedData$YrSold-completedData$YearRemod 

##Drop Id
completedData <- completedData[-1] 
```

*Additional Features Test Set*
```{r Test add feat}
#total floor area
completedDataTest$FlrSF <- sum(completedDataTest$X1stFlrSF, completedDataTest$X2ndFlrSF)

#total number of full bathrooms
completedDataTest$TotFullBath <- sum(completedDataTest$BsmtFullBath, completedDataTest$FullBath) 

#total number of half bathrooms
completedDataTest$TotSmallBath <- sum(completedDataTest$BsmtHalfBath, completedDataTest$HalfBath)

#recoding values
levels(completedDataTest$CentralAir) <- c(0,1)
completedDataTest$CentralAir <- as.numeric(as.character(completedDataTest$CentralAir))
#converting to numeric
completedDataTest$BsmntUnf <- completedDataTest$BsmtUnfSF/completedDataTest$TotalBsmtSF 

#Unfinished basement area
completedDataTest$BsmntUnf[is.na(completedDataTest$BsmntUnf)] <- 0

#Unfinished house area
completedDataTest$AreaUnf <- completedDataTest$LowQualFinSF/completedDataTest$X1stFlrSF+completedDataTest$X2ndFlrSF 

#Garage and house built at the same time
completedDataTest$TotBuild <- completedDataTest$GarageYrBlt-completedDataTest$YearBuilt

#Total Completed House Area
completedDataTest$TotArea <- sum(completedDataTest$BsmtFinSF1, completedDataTest$BsmtFinSF2, completedDataTest$LotArea, completedDataTest$TotalBsmtSF, completedDataTest$GrLivArea)

#Age of the house when sold
completedDataTest$Age <- completedDataTest$YrSold-completedDataTest$YearBuilt

#Years since remodeling until the house was sold
completedDataTest$RemodSold <- completedDataTest$YrSold-completedDataTest$YearRemod 

##Drop Id
completedDataTest <- completedDataTest[-1] 
```

#Feature Selection 

##Direct Techniques
###Forward Stepwise Selection

Forward Stepwise Selection is often used to generate an initial screening of the candidate variables when a large pool of variables exists. As this is the case for our housing dataset such selection procedure makes sense. One reasonable method would is to deploy such forward selection procedure to obtain the best ten to fifteen variables and then apply the all-possible algorithm to the variables within this subset.
*This process might take some time to run*

```{r forward}
#Get olsrr package if not already installed
library(olsrr)
model <- lm(SalePrice ~ Fireplaces + Neighborhood + OverallQual + GrLivArea + Age +  KitchenQual + ExterQual + BsmtQual + TotalBsmtSF + YearRemodAdd + CentralAir + HeatingQC, data = completedData)
K = ols_all_subset(model)
plot(K)

```

Now we will check for the best subsets of features as well as the importance of the features.

```{r best subset}
B = ols_best_subset(model)
plot(B)
```
We will focus on R^2 in order to select the best features and the output reveals that from the included features we should use *OverallQual GrLivArea MSSubClass TotalBsmtSF Neighborhood CentralAir HeatingQC KitchenQual* which yields an R^2 of 8705 and AIC of -1419.2022. AIC provides a means for model selection. Given a collection of models for the data, AIC estimates the quality of each model. Plotting the results illsutrates a similar picture that the marginal gain in adding features after the above added 6 features is minor. 

##Indirect Techniques
###Recursive Feature Selection - Wrappers

*Edit*Wrapper methods consider the selection of a set of features as a search
problem, where different combinations are prepared, evaluated and
compared to other combinations.A predictive model is used to evaluate each combination of features and assign a score based on model accuracy.

```{r}
size_sets = seq(1,length(training)-1, 5)
control <- rfeControl(functions = rfFuncs, method='cv', number=5)
results <- rfe(training[names(training) != 'SalePrice'], training$SalePrice, sizes = size_sets, rfeControl = control)
# print(results)
# predictors(results)
plot(results, type=c('g','o'))
```


##Tree Based Methods

To determine which features we want to keep and which will just add noise to our predictive model we will now grow trees. A Japanese Forest full of Bonsai. Well not literally but we will - similar to a Bonsai -  prune the tree at some point in order to reduce our RMSE. This does not only make our tree prettier but also more accurate and will eventually reflect in a higher technical quality of selected features. In a secondary step, we will use *Boosting* for the same purpose of selecting variables containing information that will make our model more accurate. For this, we will use our clean training dataset. 

###Random Forest

```{r tree, message=FALSE}
library(MASS)
library(tree)
set.seed (1)
train = sample(1:nrow(completedData), nrow(completedData)/2)
tree.house=tree(SalePrice~ .,completedData ,subset=train)
summary(tree.house)

#Fitting the random forest 
plot(tree.house)
text(tree.house ,pretty=0)
```


```{r tree check prune}
#Check where to prune the tree
cv.house=cv.tree(tree.house)
plot(cv.house$size ,cv.house$dev ,type='b')
```

The plot indicates that there is a marginal gain in reducing the RMSE by pruning the tree between 6 and 9. Consequently, we will play around with those values and observe, how many levels will give us the best result. Having tested all values between 6 and 9 the best result has proved to be 7 nodes.

```{r tree prune}
prune.house=prune.tree(tree.house ,best=7)
plot(prune.house)
text(prune.house ,pretty=0)
summary(prune.house)
```

Based on our beautiful tree, here are the features according to their importance:

*OverallQual:
+Overall material and finish quality
*Neighborhood: 
+Physical locations within Ames city limits
*GrLivArea:
+Above grade (ground) living area square feet

###Crossvalidation
To cross-validate, we will use the test dataset and observe the RMSE. 
```{r tree cross-validate}
yhat=predict(tree.house ,newdata=completedDataTest[-train ,])
house.test=completedData[-train,c("SalePrice")]
plot(yhat,house.test)
abline(0,1)
mean((yhat-house.test)^2)
```

Using regression tree techniques yields an RMSE of 0.2814337. Since we only used a few features (3 out of 81) in order to derive this tree, it is not surpirsing that we were not able to capture a good predictive model of the SalePrice by using only a fraction of features that are at our disposal from the entire dataset. Hence, we will keep those results as reference for feature selection in order to plug them into a more sophisticated linear model at a later stage.


###Boosting

This section has been inspired by the book "Introduction to Statistical Learning" by Robert Tibshirani and Trevor Hastie and this Rbloggers [post]https://www.r-bloggers.com/gradient-boosting-in-r/.

The boosted model we will use is a Gradient Boosted Model which generates 10000 trees and the shrinkage parameter (\lambda= 0.01\) describing the learning Rate. Another parameter is the interaction depth which explains the total number of splits we want to achieve. In this case, each tree is a small tree with only 4 splits. The summary of the model provides a feature importance plot where, on the top is the most important variable and at last is the least important variable. In our case, the most important feature is *OverallQual* which we already identified in the regression tree previously.

```{r tree boosting, message=FALSE}
library(gbm)
set.seed (1)
boost.house=gbm(SalePrice ~.,data=completedData[train,],distribution ="gaussian",n.trees=5000, interaction.depth=4, shrinkage = 0.01)
#call summary 
summary(boost.house)
```


```{r tree boost plot}
#Plot of Response variable with OverallQual variable
plot(boost.house,i="OverallQual") 
#Inverse relation with OverallQual  variable

plot(boost.house,i="TotalBsmtSF") 
#as the average number of rooms increases the the price increases
```

The above plot shows the relation between the variables in the x-axis and the mapping function \(f(x)\) on the y-axis. The first plot shows that OverallQual as well as TotalBsmtSF are positively correlated with the response variable SalePrice.

```{r tree predmatrix, message=FALSE}
n.trees = seq(from=100 ,to=10000, by=100) #no of trees-a vector of 100 values 

#Generating a Prediction matrix for each Tree
predmatrix<-predict(boost.house,completedData[-train,],n.trees = n.trees)
dim(predmatrix) #dimensions of the Prediction Matrix

#Calculating The Mean squared Test Error
test.error<-with(completedData[-train,],apply( (predmatrix-SalePrice)^2,2,mean))
head(test.error) #contains the Mean squared test error for each of the 100 trees averaged

#Plotting the test error vs number of trees
plot(n.trees , test.error , pch=10,col="blue",xlab="Number of Trees",ylab="Test Error", main = "Perfomance of Boosting on Test Set")

#Adding the RandomForests Minimum Error line trained on same data and similar parameters
abline(h = min(test.error),col="red") 

#Test.err is the test error of a Random forest fitted on same data
legend("topright",c("Minimum Test error Line for Random Forests"),col="red",lty=0.5,lwd=0.5)
```

The plot indicates the least error obtained from training a Random Forest (red line) .Boosting outperforms Random Forests on same test dataset with lesser Mean squared Test Errors in an interval between c.a. 400-800 trees. As the number of trees exceeds this interval, Random Forest will yield better results in terms of obtaining the least test error. However, increasing the number of trees will lead to overfitting hence, we prefer boosting over Random Forest in this scenario and choose to grow between 400 and 800 trees. 

```{r tree predmatrix 400}
n.trees = seq(from=400 ,to=1000, by=50) #no of trees-a vector of 100 values 

#Generating a Prediction matrix for each Tree
predmatrix<-predict(boost.house,completedData[-train,],n.trees = n.trees)
dim(predmatrix) #dimensions of the Prediction Matrix

#Calculating The Mean squared Test Error
test.error<-with(completedData[-train,],apply( (predmatrix-SalePrice)^2,2,mean))
head(test.error) #contains the Mean squared test error for each of the 100 trees averaged

#Plotting the test error vs number of trees
plot(n.trees , test.error , pch=10,col="blue",xlab="Number of Trees",ylab="Test Error", main = "Perfomance of Boosting on Test Set")

#adding the RandomForests Minimum Error line trained on same data and similar parameters
abline(h = min(test.error),col="red") 

#test.err is the test error of a Random forest fitted on same data
legend("topright",c("Minimum Test error Line for Random Forests"),col="red",lty=0.5,lwd=0.5)

dim(predmatrix)

head(test.error)
```

#Modeling

Wow, a lot of interesting steps we did until now but one might ask - what for? If you remember, our intial aim was to predict the SalePrice of houses in Ames, Iowa. For this, we fist cleaned, transformed and enriched the data trhough feature engineering. In a second step we used feature selection methods that would provide us with the information content of our explanatory variables. With all this new (and pretty awesome) info, we will now proceed to embed our knowledge into an actual predictive model. Hence, we will first split out dataset into training, test and validation. This is useful in order to make ensure a general yet, explanatory model and avoid bias. 

So put your seatbelts on and let's go!

##Test Training Split
```{r Split, message=FALSE}
library(arm)

splitdf <- function(dataframe, seed=NULL) {
  if (!is.null(seed)) set.seed(seed)
 	index <- 1:nrow(dataframe)
 	trainindex <- sample(index, trunc(length(index)/1.5))
 	trainset <- dataframe[trainindex, ]
 	testset <- dataframe[-trainindex, ]
 	list(trainset=trainset,testset=testset)
}

splits <- splitdf(completedData, seed=1)
training <- splits$trainset
validation <- splits$testset

set.seed(121)
train_control_config <- trainControl(method = "repeatedcv", 
                       number = 5, 
                       repeats = 1,
                       returnResamp = "all")
```

#Linear Regression Model

```{r lm}
m1 <- glm(formula=SalePrice ~ OverallQual + (OverallQual)^2 + Age + Neighborhood + GrLivArea + (GrLivArea)^2 + TotalBsmtSF + X1stFlrSF + BsmtFinSF1 + GarageArea + MoSold + CentralAir + GarageCars + OverallCond + RemodSold + LotArea + Exterior2nd + SaleCondition + GrLivArea:LotArea + Refurb:Age +  OverallCond:Age, family=gaussian, data=training)


for (x in names(validation)) {
  m1$xlevels[[x]] = union(m1$xlevels[[x]],
    levels(validation[[x]]))
}

#SANITY CHECKS IF THE CODE IS SAFE 
m1.pred = predict(m1, validation)
m1.pred[is.na(m1.pred)] = 0


my_data=as.data.frame(cbind(predicted=m1.pred,observed=validation$SalePrice))

paste("Full Linear Regression RMSE = ", sqrt(mean((m1.pred - validation$SalePrice)^2)))

m1.pred <- predict(m1, completedDataTest)

sol <- data.frame(Id=dft$Id, SalePrice=exp(m1.pred))
write.csv(sol, file="solution.csv", row.names=F)

ggplot(my_data,aes(predicted,observed))+
  geom_point() + geom_smooth(method = "lm") +
  labs(x="Predicted") +
  ggtitle('Linear Model')

```

#Conclusion

If we were again to ask a home buyer to explain their dream house, they would probably not guess anymore but reach out to us and ask for a prediction of the house they want to buy based on certain information they have. So, in the end, what can we call potential house buyers?

There is no ultimate model that will determine 100% accurately the price of a house.

Using only 16 variables we were able to explain almost 90% of the information contained in the SalePrice with an R2
 of 88%.

Our analysis yielded an RMSE of residual mean squared error of 13-14.5%, varying on the input data. RMSE can be interpreted as the standard deviation of the unexplained variance and has the useful property of being in the same units as the response variable. Lower values of RMSE indicate better fit. RMSE is a good measure of how accurately the model predicts the response and is the most important criterion for fit if the main purpose of the model is a prediction.

Modeling is an iterative process that follows a strict structure with back- and forward testing. Using tree-based methods, alongside direct and indirect feature selection techniques we were able to come up with better results.

Feature engineering reveals patterns hidden in the data. We accomplished to build a good model using new features in their interaction such as Refurb:Age and OverallCond:Age.

6.Data cleaning is no fun but there are ways, such as plots, to identify critical variables and clean them manually and let pre-built packages take care of less critical variables. This saves time and is more efficient.

#Last Note
Good, there are aspiring data scientists like us because they will make the life of fraudulent real estate agents harder and help to bring transparency to the housing market.

Thank you for reading our analysis and we hope you had more than just one beer to make it so far!

Cheers!